# '''
# '''
# first step:
# pre-train the coarse model (i.e.ğ¸ğ‘) for two epochs with a batch size of 64,
# with ğœ†ğ‘™ğ‘šğ‘˜=1ğ‘’âˆ’4, ğœ†ğ‘’ğ‘¦ğ‘’=1.0, ğœ†ğœ·=1ğ‘’âˆ’4, and ğœ†ğ=1ğ‘’âˆ’4

# Why:
# training with only lmk loss for good initialization,
# because the use of photometric loss needs good initialization both in regression and optimization
# and also, photometric loss needs differentiable rendering that makes the training slow
#
#
# '''
#output_dir: "Training/pretrain3_1024_i"
#output_dir: "Training/pretrain1_236"
#output_dir: "Training1_videoC_OnlyE/pretrain1X1"
output_dir: "/media/cine/de6afd1d-c444-4d43-a787-079519ace719/HJCode2/AUSequence/Training1_videoC_OnlyE_AULossF/pretrainXXX"
pretrained_modelpath: ''
#model_path_HJ: '/home/cine/Documents/HJCode/GANE_code/Training/testGATE30/model.tar'
dataset:
  batch_size: 1
  K: 1
loss:
  #  expression: 0.0
  #  eyed: 0.3
  #  id: 0.0
  #  lipd: 0.05
  #  lipread: 0.0
  #  lmk: 2.0
  #  lmk_mp: 2.0
  #  photo: 0.0
  #  reg_exp: 0.001
  #  reg_jaw_pose: 0.0
  #  reg_light: 0.0
  #  reg_shape: 0.0001
  #  reg_tex: 0.0
  #  relaL: 0.2
  expression: 0.8
  eyed: 0.05
  lipd: 0.0
  lmk: 0.7
  lmk_mp: 1.0
  photo: 0.0
  reg_exp: 1e-04
  reg_jaw_pose: 0.0
  #  reg_light: 0.0
  reg_shape: 0.0
  #  reg_tex: 0.
  #  reg_light: 0.
  reg_tex: 0.
  reg_light: 0.
  #  reg_tex: 0.0
  relaL: 0.08
  AUFLoss: 1.
  mainAU: 0.5
  subAU: 0.25
  weightedAU: False
  focalAU: False
train:
  resume: True
  max_epochs: 25
  max_steps: 1000000000000000000
  log_steps: 1000
  vis_steps: 1000
  checkpoint_steps: 500
  #  val_steps: 500
  #  eval_steps: 1000
  #  lr: 1e-4
  lr: 1e-4
model:
  use_tex: True